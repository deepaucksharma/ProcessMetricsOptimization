# NRDOT Process-Metrics Optimization

A production-grade, **multi-layer** process-metrics pipeline built as a custom **New Relic Distribution of OpenTelemetry (NRDOT)** collector.
Its core mission is to **significantly reduce metric ingest costs (aiming for ‚â• 90% reduction)** by intelligently refining process metric data, while preserving deep visibility into the processes that are critical for operational insight.

The pipeline will achieve this by progressively **Tagging ‚Üí Filtering ‚Üí Aggregating ‚Üí Sampling** process metrics through a series of custom OpenTelemetry processors (L0 ‚Äì L3). This project begins with a foundational "Hello World" processor (Phase 0) and will incrementally build out the full "Opt-Plus" optimization pipeline in subsequent phases.

---

## Project Status & Roadmap Snapshot

The project is developed in phases. **Currently, Phases 0 and 1 are complete.**

| Phase | Focus                                          | Status        |
|-------|------------------------------------------------|---------------|
| 0     | Foundation: Hello World processor & Local Dev Stack | ‚úÖ **Complete** |
| 1     | **L0: PriorityTagger** ‚Äì Tag critical processes  | ‚úÖ **Complete** |
| 2     | **L1: AdaptiveTopK** ‚Äì Keep busiest K processes | ‚è≥ Pending     |
| 3     | **L2: OthersRollup** ‚Äì Aggregate the rest     | ‚è≥ Pending     |
| 4     | **L3: ReservoirSampler** ‚Äì Sample the long tail  | ‚è≥ Pending     |
| 5     | Full "Opt-Plus" Pipeline (Hot + Cold Paths)    | ‚è≥ Pending     |

Detailed milestones and technical plans for each phase are documented in [IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md).

---

## Quick Start

### Dockerized OpenTelemetry Collector

This option builds and runs the actual custom OpenTelemetry collector (which includes the "Hello World" processor) within a Dockerized local development environment.

Prerequisites:
- Docker Engine installed and running.
- Docker Compose installed.

Steps:

```bash
# 1. Clone the repository (if you haven't already)
git clone https://github.com/newrelic/nrdot-process-optimization.git
cd nrdot-process-optimization

# 2. Build the custom OpenTelemetry Collector Docker image
#    (This uses build/Dockerfile and targets cmd/collector/main.go)
make docker-build

# 3. Start the local development stack using Docker Compose
#    (This uses build/docker-compose.yaml and config/base.yaml)
make compose-up

# 4. To view logs from all services (Collector, Mock Sink, etc.):
#    (Run in a new terminal or after detaching from compose-up if it wasn't run with -d)
make logs

# 5. When finished, stop the local development stack:
make compose-down
```

Local Development Stack Service URLs (when make compose-up is running):
| Service | URL | Purpose |
|---------|-----|---------|
| Collector zPages | http://localhost:15679 | Debugging collector internals (pipelines, etc.) |
| Prometheus UI | http://localhost:19090 | Querying metrics (collector & custom) |
| Grafana UI | http://localhost:13000 | Visualizing metrics (login: admin/admin) |
| Mock OTLP Sink Logs | View via make logs | Verifying OTLP data exported by the collector |

## "Hello World" Processor (Phase 0 Deliverable)

The initial phase of this project includes a fully functional "Hello World" processor integrated into the custom OpenTelemetry Collector. Its purpose is to demonstrate:

1. The fundamental structure of a custom OpenTelemetry processor (factory.go, config.go, processor.go).
2. Loading and using processor-specific configuration (from config/base.yaml).
3. Manipulating pmetric.Metrics data by adding custom attributes.
4. Core self-observability:
   - Using the standard obsreport package for common processor metrics.
   - Emitting custom processor-specific metrics (e.g., counters, gauges) via go.opentelemetry.io/otel/metric.

Functionality: As configured in config/base.yaml, the helloworld processor adds a hello.processor attribute (with a value like "Hello from NRDOT Process-Metrics Optimization!") to every metric data point. It can also be configured (add_to_resource: true) to add this attribute at the resource level.

Key Self-Metrics (visible in Prometheus/Grafana via the local dev stack):
| Metric Name | Type | Description |
|-------------|------|-------------|
| otelcol_processor_helloworld_processed_metric_points | Counter | Standard obsreport: Count of metric points processed. |
| nrdot_helloworld_mutations_total | Counter | Custom metric: Total number of metric points modified. |
| otelcol_processor_helloworld_latency_bucket | Histogram | Standard obsreport: Latency of processing. |

## "PriorityTagger" Processor (Phase 1 Deliverable)

The second phase implements the first layer (L0) of our optimization pipeline - the `prioritytagger` processor. This processor:

1. Identifies and tags process metrics that are considered critical based on various criteria
2. Preserves full visibility for these critical processes in subsequent pipeline stages
3. Serves as the foundation for the selective optimization approach

Key Features:
- Tag processes as critical based on:
  - Exact executable name matches (e.g., "systemd", "kubelet")
  - Regular expression pattern matches (e.g., "kube.*", "docker.*")
  - CPU utilization thresholds
  - Memory RSS thresholds
- Add configurable attributes to mark critical processes (default: "nr.priority=critical")
- Support for both integer and double type metric values
- Comprehensive test coverage and observability

Configuration (base.yaml example):
```yaml
processors:
  prioritytagger:
    critical_executables:
      - systemd
      - kubelet
      - dockerd
      - containerd
    critical_executable_patterns:
      - ".*java.*"
      - ".*node.*"
    cpu_steady_state_threshold: 0.5
    memory_rss_threshold_mib: 500
    priority_attribute_name: "nr.priority"
    critical_attribute_value: "critical"
```

Key Self-Metrics:
| Metric Name | Type | Description |
|-------------|------|-------------|
| otelcol_otelcol_processor_prioritytagger_processed_metric_points | Counter | Standard obsreport: Count of metric points processed. |
| otelcol_processor_dropped_metric_points | Counter | Standard obsreport: Count of metric points dropped due to errors. |

Note: The metrics naming follows the OpenTelemetry collector conventions, with the collector adding a prefix to standard metrics. Custom metrics like `nrdot_prioritytagger_critical_processes_tagged_total` require additional Prometheus configuration for visibility.

For detailed documentation, see [processors/prioritytagger/README.md](processors/prioritytagger/README.md).

## Local Development Environment Deep Dive

The make compose-up command, utilizing build/docker-compose.yaml, orchestrates a multi-container environment:

1. **Custom OpenTelemetry Collector** (otel-collector service):
   - Built using build/Dockerfile, which compiles cmd/collector/main.go.
   - Configured by config/base.yaml, which defines a pipeline:
     - hostmetrics receiver (collecting process metrics, CPU, memory, etc.).
     - prioritytagger processor (our L0 processor for tagging critical processes).
     - memory_limiter processor.
     - attributes processor (adds Hello World message).
     - batch processor.
     - Exporters: otlphttp (to the mock sink) and prometheus (for its own metrics).
   - Its zPages are exposed on http://localhost:15679 (mapped from port 55679 in the container).
   - Its Prometheus-compatible metrics endpoints are scraped by the Prometheus service.

2. **Mock OTLP Sink** (mock-otlp-sink service):
   - A simple service that listens for OTLP/HTTP metric data.
   - It logs the received metrics to standard output, allowing developers to inspect the data processed and exported by the collector. View these logs with make logs.

3. **Prometheus** (prometheus service):
   - Pre-configured to scrape metrics from:
     - The custom Collector's telemetry metrics endpoint (e.g., otel-collector:8888/metrics inside the Docker network).
     - The custom Collector's Prometheus exporter endpoint (e.g., otel-collector:8889/metrics inside the Docker network).
   - Accessible at http://localhost:19090.

4. **Grafana** (grafana service):
   - Pre-configured with Prometheus as a data source.
   - Automatically provisions the "NRDOT Custom Processor Starter KPIs" dashboard from dashboards/grafana-nrdot-custom-processor-starter-kpis.json.
   - Accessible at http://localhost:13000.

## Development Workflow & Key Makefile Targets

The Makefile is the primary entry point for most development tasks.
| Target | Action |
|--------|--------|
| make help | Displays a list of all available targets and their descriptions. |
| make build | Builds the custom OpenTelemetry collector binary (./bin/otelcol). |
| make test | Runs all tests: unit, integration (future), e2e (future), URL checks. |
| make test-unit | Runs only Go unit tests (go test ./...). |
| make test-urls | Runs test/url_check.sh to verify local dev stack service URLs. |
| make lint | Runs go vet, go fmt, and golangci-lint (if installed). |
| make docker-build | Builds the Docker image for the custom collector. |
| make compose-up | Starts the local development stack via Docker Compose. |
| make compose-down | Stops the local development stack. |
| make logs | Follows logs from all services in the Docker Compose stack. |
| make run | Starts services using run.sh docker up (alternative to compose-up). |
| make sbom | Generates a Software Bill of Materials (SBOM) using Syft (if installed). |
| make vuln-scan | Runs Go dependency vulnerability scanning (govulncheck). |
| make clean | Removes build artifacts. |

## Project Structure Overview

```
üìÅ nrdot-process-optimization/
‚îú‚îÄ‚îÄ .github/                            # GitHub Actions CI/CD Workflows
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ build.yml                   # Main CI workflow
‚îú‚îÄ‚îÄ build/                              # Files related to building the collector
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                      # For building the custom collector Docker image
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yaml             # For the local development stack
‚îú‚îÄ‚îÄ cmd/collector/                      # Entry point for the custom OpenTelemetry Collector
‚îÇ   ‚îî‚îÄ‚îÄ main.go                         # Registers processors, receivers, exporters
‚îú‚îÄ‚îÄ config/                             # Collector configuration files
‚îÇ   ‚îî‚îÄ‚îÄ base.yaml                       # Configuration for Phase 0 (hostmetrics + helloworld)
‚îÇ   ‚îî‚îÄ‚îÄ (opt-plus.yaml)                 # (Future) Configuration for the full L0-L3 pipeline
‚îú‚îÄ‚îÄ dashboards/                         # Grafana dashboard JSON definitions
‚îÇ   ‚îî‚îÄ‚îÄ grafana-nrdot-custom-processor-starter-kpis.json
‚îú‚îÄ‚îÄ docs/                               # Project documentation
‚îÇ   ‚îú‚îÄ‚îÄ DEVELOPING_PROCESSORS.md        # Guide for creating new custom processors
‚îÇ   ‚îî‚îÄ‚îÄ NRDOT_PROCESSOR_SELF_OBSERVABILITY.md # Standards for processor metrics
‚îú‚îÄ‚îÄ examples/                           # Example code directory
‚îú‚îÄ‚îÄ processors/                         # Custom OpenTelemetry processors
‚îÇ   ‚îî‚îÄ‚îÄ helloworld/                     # Phase 0: Example "Hello World" processor
‚îÇ   ‚îî‚îÄ‚îÄ prioritytagger/                 # Phase 1: L0: Critical process tagging
‚îÇ   ‚îî‚îÄ‚îÄ (adaptivetopk/)                 # (Future) L1: Top-K process selection
‚îÇ   ‚îî‚îÄ‚îÄ (othersrollup/)                 # (Future) L2: Non-priority/top process aggregation
‚îÇ   ‚îî‚îÄ‚îÄ (reservoirsampler/)             # (Future) L3: Statistical sampling
‚îú‚îÄ‚îÄ test/                               # Test suites and helper scripts
‚îÇ   ‚îî‚îÄ‚îÄ url_check.sh                    # Script to check local dev stack service URLs
‚îÇ   ‚îî‚îÄ‚îÄ (integration/)                  # (Future) Integration tests
‚îÇ   ‚îî‚îÄ‚îÄ (e2e/)                          # (Future) End-to-end tests
‚îú‚îÄ‚îÄ CLAUDE.md                           # Guidance for AI-assisted development
‚îú‚îÄ‚îÄ IMPLEMENTATION_PLAN.md              # Detailed project implementation roadmap
‚îú‚îÄ‚îÄ Makefile                            # Makefile for development tasks
‚îú‚îÄ‚îÄ go.mod                              # Go module definition
‚îú‚îÄ‚îÄ go.sum                              # Go module checksums
‚îú‚îÄ‚îÄ README.md                           # This file
‚îî‚îÄ‚îÄ run.sh                              # Unified script for running Docker stack
```

(Parentheses () indicate planned or future components not yet fully implemented in Phase 0.)

## Configuration Highlights

### config/base.yaml (Current)

This file configures the collector for the PriorityTagger and Hello World demonstration in local development:

- **Receivers**:
  - hostmetrics: Collects process metrics, CPU, memory, disk, network, etc., from the host where the collector runs. collection_interval is configurable.

- **Processors**:
  - memory_limiter: Prevents the collector from consuming excessive memory.
  - prioritytagger: Our custom L0 processor that identifies and tags critical processes.
  - helloworld: Our custom demonstration processor that adds attributes to metrics.
  - batch: Batches metrics before exporting, configured with send_batch_size and timeout.

- **Exporters**:
  - otlphttp: Exports metrics to an OTLP/HTTP endpoint. In docker-compose.yaml, this is typically re-routed to the local mock-otlp-sink service. For actual New Relic export, NEW_RELIC_LICENSE_KEY and the correct NEW_RELIC_OTLP_ENDPOINT are required.
  - prometheus: Exposes collector metrics on an endpoint (e.g., 0.0.0.0:8889) that the Prometheus service scrapes.

- **Service Pipelines**: Defines how data flows from receivers through processors to exporters.

- **Environment Variable Overrides**: Many key parameters (e.g., COLLECTION_INTERVAL, NEW_RELIC_LICENSE_KEY, OTLP endpoint) can be set via environment variables, with defaults specified in base.yaml.

### config/opt-plus.yaml (Future)

This configuration file will be developed in later phases. It will define the full, multi-layer optimization pipeline:
hostmetrics -> L0:PriorityTagger -> L1:AdaptiveTopK -> L2:OthersRollup -> L3:ReservoirSampler -> batch -> otlphttp (to New Relic)
It may also include a "cold path" exporter for raw data (e.g., to Parquet/S3) for archival or deep analysis.

## The Optimization Pipeline

The core of this project is the development of a series of custom OpenTelemetry processors designed to work in concert:

1. **L0: PriorityTagger** ‚úÖ: Identifies and tags metrics belonging to critical processes (e.g., based on executable name, regex patterns, CPU utilization, or memory usage). Tagged metrics can bypass or receive special handling in subsequent layers. Supports both integer and double metric value types for proper threshold comparison.

2. **L1: AdaptiveTopK** ‚è≥: Selects metrics from the top 'K' most resource-intensive processes (e.g., by CPU or memory). The value of 'K' can be dynamically adjusted based on overall host load.

3. **L2: OthersRollup** ‚è≥: Aggregates metrics from all other non-priority, non-TopK processes into a single summary series (e.g., _other_ process). This drastically reduces cardinality while retaining a sense of overall system load from less significant processes.

4. **L3: ReservoirSampler** ‚è≥: Applies statistical sampling (e.g., reservoir sampling) to the remaining long-tail of processes not captured by L0, L1, or L2. This provides representative insight into a subset of these processes without exporting all of them.

## Contributing

Contributions are highly welcome!

- To understand the current development focus and upcoming features, please review the [IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md).
- For guidance on developing new custom processors, see docs/DEVELOPING_PROCESSORS.md.
- For standards on processor self-observability and metrics, refer to docs/NRDOT_PROCESSOR_SELF_OBSERVABILITY.md.
- If using AI for assistance, CLAUDE.md provides project-specific context.

Please feel free to open issues for bugs, feature requests, or ideas. Pull requests are encouraged.